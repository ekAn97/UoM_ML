# -*- coding: utf-8 -*-
"""Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a2NfBjyUaejLLeHeHHSrLPTWZcw0BqeB
"""

# Generic libraries
import pandas as pd
import seaborn as sb
import numpy as np
import matplotlib.pyplot as plt
import random
import copy
import warnings
from google.colab import drive
from google.colab import files

# ML libraries
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score,\
recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier

#############################################################################
#                       C U S T O M  F U N C T I O N S                      #
#############################################################################

def df_stats(df, indices):
    '''
    Calculates the min, max and average
    values of the columns of a dataframe

    INPUT
    -----
    pd.DataFrame

    OUTPUT
    ------
    pd.DataFrame with min, max and avg values for each predictor

    '''
    df_stats = pd.DataFrame({
        "Min": [min(df[_]) for _ in df.columns.values],
        "Max": [max(df[_]) for _ in df.columns.values],
        "Avg": [sum(df[_]) / len(df[_]) for _ in df.columns.values]
        }, index = indices)

    return df_stats

def create_legend_table(ax, df, bbox_to_anchor=(1.05, 0.65, 0.3, 0.25),
                        font_size=8):
    ''' Function to create a nice legend
    and place it outside of plot

    INPUT
    -----
    ax: figure
    df: pd.DataFrame

    '''
    legend_table_data = []
    for x, min_val, max_val, avg_val in zip(df.columns, df["Min"], df["Max"],\
                                            df["Avg"]):
        legend_table_data.append([x, f"{min_val:.2f}", f"{max_val:.2f}",\
                                  f"{avg_val:.2f}"])

    column_labels = ["Predictor", "Min", "Max", "Avg"]

    # Create a legend table outside the plot
    legend_table = ax.table(cellText=legend_table_data,\
                            colLabels=column_labels,loc="center left",\
                                bbox=bbox_to_anchor)

    # Set font size
    legend_table.auto_set_font_size(False)
    legend_table.set_fontsize(font_size)

    # Remove borders and background color for a cleaner look
    for key, cell in legend_table.get_celld().items():
        cell.set_linewidth(0)
        cell.set_edgecolor("none")

def plot_error_bar(ax, df, title: str):
    ''' Function that plots error
    bars for the statistical summaries of
    a predictor

    INPUT
    -----
    ax: figure
    df: pd.DataFrame
    title: Title of the figure

    OUTPUT
    ------
    figure
    '''
    # Creating errorbar
    ax.errorbar(df.index, df["Avg"],\
                yerr=[df["Avg"] - df["Min"], df["Max"] - df["Avg"]],\
                    fmt="o", color="orange", markersize=8, capsize=5)
    ax.set_title(title)
    ax.set_xlabel("Predictors")
    ax.set_ylabel("Values")
    ax.set_xticklabels(df.index, rotation=0, ha="right")

    # Add legend using the customized function
    create_legend_table(ax, df)

def cmatrix_plot(cf_matrix, class_names):
    '''
    Function to create a confusion matrix

    INPUT
    -----
    cf_matrix: confusion matrix
    class_names: list of the form ["a", "b", "c", etc]

    OUTPUT
    ------
    Shows plot of confusion matrix
    '''
    group_counts = ["{0:0.0f}".format(value)\
                   for value in cf_matrix.flatten()/np.sum(cf_matrix)]

    group_percentages = ["{0:.2%}".format(value)\
                         for value in cf_matrix.flatten()/np.sum(cf_matrix)]

    labels = [f"{v1}\n{v2}\n" for v1, v2 in zip(group_counts,\
                                                group_percentages)]

    labels = np.asarray(labels).reshape(2, 2)

    ax = sb.heatmap(cf_matrix, annot=labels, fmt="", cmap="Blues")

    ax.set_title("Seaborn Confusion Matrix with labels\n\n")
    ax.set_xlabel("\nPredicted Status Category")
    ax.set_ylabel("Actual Status Category")

    # Ticket labels-alphabetical order
    ax.xaxis.set_ticklabels(class_names)
    ax.yaxis.set_ticklabels(class_names)

    # Display
    plt.show()

def eval_metrics(y0, y_pred, set_type: str, classifier: str):
    '''Function that calculates evaluation metrics
    and confusion matrix

    INPUT
    -----
    y0: Actual response values
    y_pred: Predicted response values
    set_type: "train" or "test"
    classifier: name of the classifier used

    OUTPUT
    ------
    pd.DataFrame with calculated metrics
    '''
    # Confusion Matrix
    cf_matrix = confusion_matrix(y0, y_pred)
    cmatrix_plot(cf_matrix, [1, 2])

    # Metrics
    print("Accuracy: {:.2f}".format(accuracy_score(y0, y_pred)))
    print("Precision: {:.2f}".format(precision_score(y0, y_pred,\
                                                     pos_label=2)))
    print("Recall: {:.2f}".format(recall_score(y0, y_pred, pos_label=2)))
    print("F1-score: {:.2f}".format(f1_score(y0, y_pred, pos_label=2)))
    ra_score = roc_auc_score(y0, y_pred)
    print("ROC-AUC: {:.2f}".format(ra_score))

    df = {"Classifier Name": classifier,\
          "Training or test set": set_type,\
              "Number of training samples": len(X_train),\
                  "Number of non-healthy companies in training sample":\
                      counts_train[1],\
                          "True negatives TN": cf_matrix[0][0],\
                              "True positive TP": cf_matrix[1][1],\
                                  "False negatives FN": cf_matrix[0][1],\
                                      "False positives FP": cf_matrix[1][0],\
                                          "ROC-AUC": ra_score}

    return df

def classifiers(model_name: str, model_function, parameters=dict()):
    '''
    Function that performs training and prediction using
    gridsearch first, to determine the optimal estimator

    INPUT
    -----
    model_name: name of the classifier
    model_function: sklearn function
    parameters: dictionary with parameters for the classifier; default = empty

    OUTPUT
    ------
    list of pd.DataFrame for training and test results
    '''
    gs = GridSearchCV(model_function, parameters)
    gs.fit(X_train, y_train)

    train_preds_grid = gs.predict(X_train)
    test_preds_grid = gs.predict(X_test)

    results_df_train = eval_metrics(y_train, train_preds_grid, "train",\
                                    model_name)
    results_df_test = eval_metrics(y_test, test_preds_grid, "test", model_name)

    # Printing Best Parameters
    parameters = gs.best_params_
    print(f"Best Parameters for {model_name}: {parameters}")

    return [results_df_train, results_df_test]

#############################################################################
#                      M A I N  S C R I P T                                 #
#############################################################################

# Warnings
warnings.filterwarnings("ignore")
# Set seed
np.random.seed(1312)
random.seed(1312)

# Accessing dataset
drive.mount('/content/drive')
# Change the location to the folder where the dataset is stored
location = "/content/drive/My Drive/Classification Project/"
file_name = "Dataset2Use_Assignment1.xlsx"

# Creating dataframe
dataset = pd.read_excel(location + file_name)

# Renaming the columns to something handier(x-value names indicate
# the predictors
mapping_dict = dict()
for _ in range(8):
    mapping_dict.update({dataset.columns.values[_]: "x{}".format(_ + 1)})
for i, j in enumerate(dataset.columns.values[8:11]):
    mapping_dict.update({j: "binary{}".format(i + 1)})

mapping_dict.update({dataset.columns.values[-2]: "Y"})
mapping_dict.update({dataset.columns.values[-1]: "year"})

dataset.rename(columns = mapping_dict, inplace=True)

# Creating DataFrame of Healthy and Bankrupt Businesses
years = dataset.year.unique()
status_dict = {"Healthy": [dataset[dataset["year"]\
                                   == _]["Y"].value_counts()[1] for _ in years],
               "Bankrupt": [dataset[dataset["year"]\
                                    == _]["Y"].value_counts()[2] for _ in years]}

status_df = pd.DataFrame(status_dict, index = years)

# Create figure (number of Healthy and Bankrupt)
fig1 = status_df.plot.bar(rot=45)
# To show the exact value of each bar
for p in fig1.patches:
    fig1.annotate(text=np.round(p.get_height()),\
                  xy=(p.get_x()+p.get_width()/2., p.get_height()), ha="center",\
                      va="center", xytext=(0, 10), textcoords="offset points")
fig1.spines["top"].set_visible(False)

plt.tight_layout()
plt.savefig(location + "Figure_1.png", dpi=300)
plt.show()

# Check for NaN values
print(dataset.isnull().sum())

# Create separate DataFrames for healthy and bankrupt businesses
healthy_df = dataset[dataset.iloc[:, -2] == 1]
bankrupt_df = dataset[dataset.iloc[:, -2] == 2]

# Getting the continuous predictor columns for the healthy and bankrupt df
predictors = dataset.columns.values[:8]
healthy_df_stats = df_stats(healthy_df.iloc[:, :8], predictors)
bankrupt_df_stats = df_stats(bankrupt_df.iloc[:, :8], predictors)

# Plot
fig, (ax1, ax2) = plt.subplots(nrows=2,figsize=(12, 8))

plot_error_bar(ax1, healthy_df_stats, "Stats for Healthy Businesses")
plot_error_bar(ax2, bankrupt_df_stats, "Stats for Bankrupt Businesses")

# Show the plot
plt.tight_layout()
plt.show()

# Separating predictor and response columns
inputValues_c = dataset.iloc[:, :8]
inputValues_b = dataset.iloc[:, 8: 11]
y_values = dataset["Y"]

# Scaling the Continuous Data
scaler_inputs = MinMaxScaler()
sc_inputValues_c = scaler_inputs.fit_transform(inputValues_c.to_numpy())
sc_inputValues_c_df = pd.DataFrame(sc_inputValues_c,\
                           columns=inputValues_c.columns)

# Input Values
inputValues = pd.concat([sc_inputValues_c_df, inputValues_b], axis=1)

# Stratified k-fold
strat_f = StratifiedKFold(n_splits=4, shuffle=False)
strat_f.get_n_splits(inputValues, y_values)

# Loop for steps 6-9
for fold, (train_index, test_index) in enumerate(strat_f.split(inputValues,\
                                                               y_values)):
    X_train, X_test = inputValues.iloc[train_index],inputValues.iloc[test_index]
    y_train, y_test = y_values.iloc[train_index], y_values.iloc[test_index]
    print(len(X_train))
    print(len(X_test))
    # Number of healthy and bankrupt business in the train and test sets
    unique_train, counts_train = np.unique(y_train.values, return_counts=True)
    unique_test, counts_test = np.unique(y_test.values, return_counts=True)

    print(f"Fold {fold}")
    print("Number of healthy businesses:", counts_train[0], "(Train) and",
          counts_test[0], "(Test)")
    print("Number of bankrupt businesses:", counts_train[1], "(Train) and",
          counts_test[1], "(Test)")
    # Ratio healthy/bankrupt
    if counts_train[0] > 3*(counts_train[1]):
        train_index_new = []
        X_train_copy = copy.deepcopy(X_train)
        y_train_copy = copy.deepcopy(y_train)

        for i in X_train.index.values:
            if y_train[i] == 2:
                train_index_new.append(i)
                X_train_copy.drop(i, axis=0, inplace=True)
                y_train_copy.drop(i, axis=0, inplace=True)

        while len(train_index_new) != 4*(counts_train[1]):
            random_index = np.random.choice(X_train_copy.index.values)
            train_index_new.append(random_index)
            X_train_copy.drop(random_index, axis=0, inplace=True)
            y_train_copy.drop(random_index, axis=0, inplace=True)
        # New train set
        X_train, y_train = inputValues.iloc[train_index_new],\
        y_values.iloc[train_index_new]
        # New test set
        X_test = pd.concat([X_test, X_train_copy])
        y_test = pd.concat([y_test, y_train_copy])

    # New distributions
    unique_train, counts_train = np.unique(y_train.values, return_counts=True)
    unique_test, counts_test = np.unique(y_test.values, return_counts=True)
    ratio_df = pd.DataFrame({"Train": [counts_train[0], counts_train[1]],\
                             "Test": [counts_test[0], counts_test[1]]},\
                            index=["Healthy", "Bankrupt"])
    # Print new distributions
    display(ratio_df)

    ### Classifiers
    # K-nn
    knn_df = classifiers("K-nn", KNeighborsClassifier(),\
                         {"n_neighbors": range(1, 50)})

    results_train_df = pd.DataFrame(knn_df[0], index=[0])
    results_test_df = pd.DataFrame(knn_df[1], index=[0])

    # Gaussian NB
    nb_df = classifiers("Gaussian NB", GaussianNB())

    results_train_df = results_train_df.append(nb_df[0], ignore_index=True)
    results_test_df = results_test_df.append(nb_df[1], ignore_index=True)

    # SVM
    svm_df = classifiers("SVM", SVC(),\
                         {'C': [0.1, 1, 10, 100],\
                          'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\
                              'kernel': ['linear', 'rbf', 'poly', 'sigmoid']})

    results_train_df = results_train_df.append(svm_df[0], ignore_index=True)
    results_test_df = results_test_df.append(svm_df[1], ignore_index=True)

    # LDA
    lda_df = classifiers("LDA", LinearDiscriminantAnalysis(),\
                         {"solver": ["svd", "lsqr", "eigen"]})
    results_train_df = results_train_df.append(lda_df[0], ignore_index=True)
    results_test_df = results_test_df.append(lda_df[1], ignore_index=True)

    # Logistic Regression
    lg_df = classifiers("Logistic Regression", LogisticRegression(),\
                        {"C":np.logspace(-3,3,7), "penalty":["l1","l2"]})
    results_train_df = results_train_df.append(lg_df[0], ignore_index=True)
    results_test_df = results_test_df.append(lg_df[1], ignore_index=True)

    # Decision Trees
    dt_df = classifiers("Decision Trees", DecisionTreeClassifier(),\
                        {'max_features': ['auto', 'sqrt', 'log2'],\
                         'ccp_alpha': [0.1, .01, .001],\
                             'max_depth' : [5, 6, 7, 8, 9],\
                                 'criterion' :['gini', 'entropy']})

    results_train_df = results_train_df.append(dt_df[0], ignore_index=True)
    results_test_df = results_test_df.append(dt_df[1], ignore_index=True)

    # Random Forest
    rf_df = classifiers("Random Forest", RandomForestClassifier(),\
                        {'n_estimators': [200, 500],\
                         'max_features': ['auto', 'sqrt', 'log2'],\
                             'max_depth' : [4,5,6,7,8],\
                                 'criterion' :['gini', 'entropy']})
    results_train_df = results_train_df.append(rf_df[0], ignore_index=True)
    results_test_df = results_test_df.append(rf_df[1], ignore_index=True)


    # Neural Network
    nn_df = classifiers("NN", MLPClassifier(),\
                        {'hidden_layer_sizes': np.arange(1, 6),\
                         'alpha': 10.0 ** (-np.arange(1, 10)),\
                             'activation': ['identity', 'logistic', 'tanh',\
                                            'relu']})

    results_train_df = results_train_df.append(nn_df[0], ignore_index=True)
    results_test_df = results_test_df.append(nn_df[1], ignore_index=True)

    # Store Results
    results_train_df.to_csv("balancedDataOutcomes_Train_Fold_{}.csv".format(fold),\
                            index=False)
    results_test_df.to_csv("balancedDataOutcomes_Test_Fold_{}.csv".format(fold),\
                           index=False)
    # Download CSV files
    files.download("balancedDataOutcomes_Train_Fold_{}.csv".format(fold))
    files.download("balancedDataOutcomes_Test_Fold_{}.csv".format(fold))
